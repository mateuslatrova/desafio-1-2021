{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Desafio 1 - Maratona Behind the Code 2021\n\n### Notebook guia\n\nEsse Jupyter Notebook te dar\u00e1 instru\u00e7\u00f5es para criar uma solu\u00e7\u00e3o introdut\u00f3ria para o desafio 1 da Maratona. Sinta-se livre para editar e melhorar sua solu\u00e7\u00e3o!\n"}, {"metadata": {}, "cell_type": "markdown", "source": "**Aten\u00e7\u00e3o: se estiver utilizando o Watson Studio, lembre-se de deixar o Notebook como edit\u00e1vel, clicando no bot\u00e3o de editar acima.**\n\n![](https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/edit-notebook.png)"}, {"metadata": {}, "cell_type": "markdown", "source": "## Explora\u00e7\u00e3o do dataset\n\nO primeiro passo para o desenvolvimento de um bom modelo de Machine Learning \u00e9 explorar bem os dados que temos para trabalhar. Devemos entender o melhor poss\u00edvel a relev\u00e2ncia de cada dado para o valor que queremos predizer. Afinal, a predi\u00e7\u00e3o do modelo \u00e9 inteiramente baseada nos dados com que treinou.\n\nExistem muitas bibliotecas em Python que podem ser utilizadas para tratamento e visualiza\u00e7\u00e3o de dados. Nesses exemplos, vamos usar Pandas, Seaborn e Matplotlib."}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Primeiramente, vamos carregar o dataset do desafio neste Notebook. Vamos come\u00e7ar com o principal, `LOANS.csv`. Para isso, podemos usar o \u00edcone de assets, dispon\u00edvel no canto superior direito da tela, e inserir o dataset como um DataFrame Pandas, como na imagem abaixo. \n\n<img width=\"300px\" src=\"https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/load-loans.png\" />\n\nRepita para todos os datasets que for utilizar."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Carregue aqui o dataset", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Renomeie o nome da vari\u00e1vel criada com o dataset para `loans`, para ficar de acordo com os c\u00f3digos abaixo."}, {"metadata": {}, "cell_type": "code", "source": "loans = df_carregado", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Podemos usar os m\u00e9todos `.info()` e `.describe()` para obter informa\u00e7\u00f5es b\u00e1sicas sobre quantidade presente dos dados, tipos e valores deles."}, {"metadata": {}, "cell_type": "code", "source": "loans.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "loans.describe()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "A vari\u00e1vel destino para este desafio \u00e9 a `ALLOW`, significando se um empr\u00e9stimo dever\u00e1 ser permitido ou n\u00e3o, baseado nas informa\u00e7\u00f5es dadas. Vamos dar uma olhada em como est\u00e1 a distribui\u00e7\u00e3o dessa vari\u00e1vel"}, {"metadata": {}, "cell_type": "code", "source": "risk_plot = sns.countplot(data=loans, x='ALLOW', order=loans['ALLOW'].value_counts().index)\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Sinta-se livre para ver a distribui\u00e7\u00e3o de outras colunas do conjunto de dados, utilizar os outros conjuntos de dados, explorar as correla\u00e7\u00f5es entre vari\u00e1veis e outros."}, {"metadata": {}, "cell_type": "code", "source": "# Adicione suas explora\u00e7\u00f5es", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Tratamento dos dados"}, {"metadata": {}, "cell_type": "markdown", "source": "Uma vez que exploramos os dados, entendemos a import\u00e2ncia de cada coluna e podemos fazer altera\u00e7\u00f5es nelas para para obter um melhor resultado. Aqui, vamos fazer apenas um tratamento simples, de remover do dataset as linhas que tiverem faltando algum valor. N\u00e3o necessariamente essa t\u00e9cnica \u00e9 a melhor para se utilizar no desafio, \u00e9 apenas um exemplo de como tratar o dataset.\n\nPara tratamentos mais avan\u00e7ados, como modifica\u00e7\u00e3o de colunas ou cria\u00e7\u00e3o de novas colunas, veja mais abaixo no Notebook, em que explicamos como utilizar as `Pipelines`, da biblioteca `sklearn`, para realizar transforma\u00e7\u00f5es nos dados."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "clean_df = loans.dropna()\nclean_df.count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Podemos observar que agora temos um dataset \"limpo\", mas perdemos alguns dados por remover as linhas em que pelo menos uma coluna estava faltando."}, {"metadata": {}, "cell_type": "markdown", "source": "Observando a execu\u00e7\u00e3o do m\u00e9todo `.info()` acima, podemos ver que existem tr\u00eas colunas do tipo `object`. O modelo do `scikit-learn` que vamos usar n\u00e3o \u00e9 capaz de processar uma vari\u00e1vel desse tipo. Portanto, para dar seguimento ao experimento, vamos remover essa coluna. Recomendamos que voc\u00ea use alguma t\u00e9cnica para tratamento de vari\u00e1veis categ\u00f3ricas, como o _one-hot encoding_, em vez de remover a coluna.\n\nVamos tamb\u00e9m remover a coluna `ID`, pois sabemos que ela n\u00e3o \u00e9 uma informa\u00e7\u00e3o \u00fatil para a predi\u00e7\u00e3o (\u00e9 apenas um n\u00famero identificando um cliente)."}, {"metadata": {}, "cell_type": "code", "source": "object_columns = ['INSTALLMENT_PLANS', 'LOAN_PURPOSE', 'OTHERS_ON_LOAN']\nclean_df = clean_df.drop(object_columns, axis=1)\nclean_df = clean_df.drop('ID', axis=1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clean_df.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Cria\u00e7\u00e3o do modelo\n\nCom os dados prontos, podemos selecionar um modelo de Machine Learning para treinar com nossos dados. Nesse exemplo, vamos utilizar um modelo de classifica\u00e7\u00e3o b\u00e1sico, o de \u00c1rvore de Decis\u00e3o.\n\nPara conseguir avaliar o desempenho do nosso modelo, vamos dividir os dados que temos entre dados de treino e de teste, e assim, ap\u00f3s o treinamento, verificar como ele est\u00e1 se saindo com as predi\u00e7\u00f5es."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Abaixo, separamos os dados que queremos predizer dos dados que utilizamos como informa\u00e7\u00f5es para a predi\u00e7\u00e3o."}, {"metadata": {}, "cell_type": "code", "source": "features = ['PAYMENT_TERM', 'INSTALLMENT_PERCENT', 'LOAN_AMOUNT']\ntarget = ['ALLOW']\n\nX = clean_df[features]\ny = clean_df[target]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "test_pct = 0.3 # Separaremos 30% dos dados para testes\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_pct)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(f\"Acur\u00e1cia do modelo (n\u00famero de predi\u00e7\u00f5es assertivas sobre n\u00famero total de testes): {acc}\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Apesar de estarmos utilizando somente algumas vari\u00e1veis do dataset carregado, o desafio espera um modelo que aceite todas as vari\u00e1veis dos conjuntos de dados dispon\u00edveis. Portanto, vamos utilizar um transformador para transformar os dados de entrada, removendo as colunas que n\u00e3o queremos, antes envi\u00e1-los ao nosso modelo. Dessa forma, criaremos uma `Pipeline`, que utiliza o transformador como entrada, e o nosso modelo em seguida.\n\nFica como tarefa para voc\u00ea unir os outros conjuntos de dados dispon\u00edveis e utiliz\u00e1-los tamb\u00e9m para predi\u00e7\u00f5es no modelo, em vez de remover as colunas."}, {"metadata": {}, "cell_type": "markdown", "source": "## Sobre Pipelines\n\nUma `Pipeline`, da biblioteca `scikit-learn`, consiste em uma s\u00e9rie de passos onde realizamos transforma\u00e7\u00f5es em dados. As transforma\u00e7\u00f5es s\u00e3o definidas por classes que devem ter sempre **dois m\u00e9todos**:\n\n- **fit**: Um m\u00e9todo que recebe dados de treinamento, e retorna a pr\u00f3pria inst\u00e2ncia da classe. Ele \u00e9 aplicado quando se vai treinar utilizar uma Pipeline para treinar um modelo.\n- **transform**: Um m\u00e9todo que recebe como entrada um conjunto de dados e deve retornar um outro conjunto de dados, transformado. Ele \u00e9 aplicado em cada etapa da Pipeline, recebendo os dados do passo anterior e transformando-os.\n\nVeja abaixo uma representa\u00e7\u00e3o gr\u00e1fica do funcionamento de uma Pipeline:"}, {"metadata": {}, "cell_type": "markdown", "source": "![](https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/pipeline.png)"}, {"metadata": {}, "cell_type": "markdown", "source": "Nesse Notebook, vamos criar uma Pipeline muito similar ao exemplo acima, com dois est\u00e1gios:\n\n- **drop_columns**: Remove as colunas indesejadas do conjunto de dados de entrada.\n- **classification**: Alimenta um modelo de classifica\u00e7\u00e3o com os dados obtidos no est\u00e1gio **drop_columns**, podendo ser tanto para treinamento quanto para obter uma predi\u00e7\u00e3o."}, {"metadata": {}, "cell_type": "markdown", "source": "## Cria\u00e7\u00e3o de Pipelines no scikit-learn\n\nPara criar um modelo capaz de fazer transforma\u00e7\u00f5es nos dados de entrada, vamos criar uma `Pipeline` do `scikit-learn` e aplicar nossas transforma\u00e7\u00f5es dentro dos est\u00e1gios dela.\n\nAbaixo, definimos um transformador exemplo, que ir\u00e1 remover as colunas passadas como par\u00e2metro em sua inicializa\u00e7\u00e3o:"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\n\n# Um transformador para remover colunas indesejadas\nclass DropColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Primeiro realizamos a c\u00f3pia do DataFrame 'X' de entrada\n        data = X.copy()\n        # Retornamos um novo dataframe sem as colunas indesejadas\n        return data.drop(labels=self.columns, axis='columns')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Tanto o m\u00e9todo `fit` quanto o `transform` devem obrigatoriamente ser definidos, mesmo se n\u00e3o forem fazer nada de diferente, como no caso do `fit` acima.\n\nDa mesma forma, voc\u00ea pode criar outros transformadores, para outros prop\u00f3sitos, sempre herdando das classes `BaseEstimator` e `TransformerMixin`. Voc\u00ea pode utilizar um transformador para, por exemplo, criar novas colunas, editar tipos de dados de colunas existentes, entre outros.\n\nAgora, vamos criar uma Pipeline para utiliza\u00e7\u00e3o do nosso modelo, aceitando todas as colunas esperadas pelo desafio e removendo as que n\u00e3o queremos usar."}, {"metadata": {}, "cell_type": "code", "source": "challenge_columns = ['ID', 'CHECKING_BALANCE', 'PAYMENT_TERM', 'CREDIT_HISTORY',\n       'LOAN_PURPOSE', 'LOAN_AMOUNT', 'EXISTING_SAVINGS',\n       'EMPLOYMENT_DURATION', 'INSTALLMENT_PERCENT', 'SEX', 'OTHERS_ON_LOAN',\n       'CURRENT_RESIDENCE_DURATION', 'PROPERTY', 'AGE', 'INSTALLMENT_PLANS',\n       'HOUSING', 'EXISTING_CREDITS_COUNT', 'JOB_TYPE', 'DEPENDENTS',\n       'TELEPHONE', 'FOREIGN_WORKER', 'ALLOW']\n\nunwanted_columns = list((set(challenge_columns) - set(target)) - set(features)) # Remover todas as colunas que n\u00e3o s\u00e3o features do nosso modelo", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Criando uma inst\u00e2ncia do transformador, passando como par\u00e2metro as colunas que n\u00e3o queremos\ndrop_columns = DropColumns(unwanted_columns)\n\n\n# Criando a Pipeline, adicionando o nosso transformador seguido de um modelo de \u00e1rvore de decis\u00e3o\nskl_pipeline = Pipeline(steps=[('drop_columns', drop_columns), ('classification', model)])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Pronto! Essa pipeline agora est\u00e1 pronta para receber todas as vari\u00e1veis do desafio, apesar de o modelo s\u00f3 usar algumas."}, {"metadata": {}, "cell_type": "markdown", "source": "## Deploy do modelo para o Watson Machine Learning (WML)\n\nAgora temos o modelo pronto para publica\u00e7\u00e3o, queremos deix\u00e1-lo online para que o sistema da Maratona possa test\u00e1-lo :)\n\nPara isso, vamos utilizar a biblioteca `IBM Watson Machine Learning`, que permite realizar o encapsulamento de modelos de Machine Learning em APIs REST.\n"}, {"metadata": {}, "cell_type": "code", "source": "# Instalar a biblioteca do WML\n!pip install -U ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Caso ainda n\u00e3o tiver criado, crie um servi\u00e7o de Machine Learning aqui: https://cloud.ibm.com/catalog/services/machine-learning.\n\nInsira suas credenciais para o servi\u00e7o na c\u00e9lula abaixo.\n\nEm `location`, insira o ID da regi\u00e3o onde se encontra o seu servi\u00e7o WML instanciado, de acordo com as possibilidades abaixo:\n\n- Dallas - `us-south`\n- London - `eu-gb`\n- Frankfurt - `eu-de`\n- Tokyo - `jp-tok`\n\nPara a API key, voc\u00ea deve ger\u00e1-la aqui: https://cloud.ibm.com/iam/apikeys. N\u00e3o compartilhe-a com ningu\u00e9m! Uma API key d\u00e1 acesso \u00e0 sua conta IBM Cloud."}, {"metadata": {}, "cell_type": "code", "source": "api_key = 'INSIRA SUA API KEY AQUI'\nlocation = 'us-south' # Caso o WML estiver em uma regi\u00e3o diferente, altere\n\nwml_credentials = {\n    \"apikey\": api_key,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Crie um espa\u00e7o para salvar o seu modelo. Voc\u00ea pode cri\u00e1-lo aqui: https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas\n\nQuando criar o seu espa\u00e7o, **associe a inst\u00e2ncia do seu servi\u00e7o WML ao espa\u00e7o!** Sem associar, voc\u00ea n\u00e3o conseguir\u00e1 efetuar o deploy."}, {"metadata": {}, "cell_type": "code", "source": "# Listar espa\u00e7os criados na sua inst\u00e2ncia de WML\nclient.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Copie o ID do seu espa\u00e7o criado para o desafio e cole-o abaixo para utiliz\u00e1-lo. Voc\u00ea dever\u00e1 ver a mensagem 'SUCCESS' se o espa\u00e7o estiver corretamente configurado."}, {"metadata": {}, "cell_type": "code", "source": "space_id = 'cole aqui'\nclient.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Utiliza\u00e7\u00e3o da Pipeline dentro do Watson Machine Learning (WML)\n\nPara utilizar uma Pipeline no WML com transformadores customizados, s\u00e3o necess\u00e1rios alguns passos adicionais:\n\n1. Criar um pacote no Python contendo o transformador customizado;\n2. Carregar esse pacote com o transformador em um reposit\u00f3rio no WML;\n3. Criar uma especifica\u00e7\u00e3o de software, com esse pacote customizado, que vai ser utilizada como tempo de execu\u00e7\u00e3o do modelo no WML.\n\nComo exemplo, vamos utilizar um pacote j\u00e1 pronto, dispon\u00edvel aqui: https://github.com/vnderlev/watson-sklearn-transforms. Para configurar o pacote Python, s\u00e3o necess\u00e1rios alguns outros arquivos, mas a l\u00f3gica do transformador criado se encontra [neste arquivo](https://github.com/vnderlev/watson-sklearn-transforms/blob/master/my_custom_sklearn_transforms/sklearn_transformers.py). No caso, esse \u00e9 o mesmo transformador que definimos aqui, ele vai excluir do conjunto de dados as colunas passadas como par\u00e2metro na sua inicializa\u00e7\u00e3o.\n\nAbaixo, vamos baixar esse pacote do GitHub e instal\u00e1-lo no Python."}, {"metadata": {}, "cell_type": "code", "source": "!rm -rf watson-sklearn-transforms # Remover a pasta caso j\u00e1 exista\n!git clone https://github.com/vnderlev/watson-sklearn-transforms # Clonar o reposit\u00f3rio com o pacote\n!zip -r drop-columns.zip watson-sklearn-transforms # Zipar o pacote\n!pip install drop-columns.zip # Instalar o pacote zipado", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Vamos agora recriar nossa Pipeline utilizando esse pacote instalado."}, {"metadata": {}, "cell_type": "code", "source": "from my_custom_sklearn_transforms.sklearn_transformers import DropColumns\n\ndrop_columns = DropColumns(unwanted_columns)\n\npipeline = Pipeline(steps=[('drop_columns', drop_columns), ('classification', model)])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Vamos agora subir o transformador customizado que baixamos para o WML."}, {"metadata": {}, "cell_type": "code", "source": "# Metadados para o pacote customizado\nmeta_prop_pkg_extn = {\n    client.package_extensions.ConfigurationMetaNames.NAME: \"Drop_Columns\",\n    client.package_extensions.ConfigurationMetaNames.DESCRIPTION: \"Extens\u00e3o para remover colunas\",\n    client.package_extensions.ConfigurationMetaNames.TYPE: \"pip_zip\"\n}\n\n# Subir o pacote\npkg_extn_details = client.package_extensions.store(meta_props=meta_prop_pkg_extn, file_path=\"drop-columns.zip\")\n\n# Salvar as informa\u00e7\u00f5es sobre o pacote\npkg_extn_uid = client.package_extensions.get_uid(pkg_extn_details)\npkg_extn_url = client.package_extensions.get_href(pkg_extn_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Vamos agora criar uma especifica\u00e7\u00e3o de software com o nosso pacote customizado, para que o WML possa utilizar. Caso estiver utilizando um software diferente de `Python 3.8` ou biblioteca diferente de `scikit-learn`, voc\u00ea pode dar uma olhada na lista de especifica\u00e7\u00f5es de software suportadas pelo WML: https://dataplatform.cloud.ibm.com/docs/content/wsj/wmls/wmls-deploy-python-types.html?context=analytics&audience=wdp"}, {"metadata": {}, "cell_type": "code", "source": "base_sw_spec_uid = client.software_specifications.get_uid_by_name(\"default_py3.8\")\n\n# Caso queira utilizar um software diferente de Python 3.8 como base, d\u00ea uma olhada nos dispon\u00edveis com a linha abaixo\n# client.software_specifications.list(limit=100)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Metadados da nova especifica\u00e7\u00e3o de software\nmeta_prop_sw_spec = {\n    client.software_specifications.ConfigurationMetaNames.NAME: \"sw_spec_drop_columns\",\n    client.software_specifications.ConfigurationMetaNames.DESCRIPTION: \"Software specification utilizando DropColumns\",\n    client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {\"guid\": base_sw_spec_uid}\n}\n\n# Criando a nova especifica\u00e7\u00e3o de software e obtendo seu ID\nsw_spec_details = client.software_specifications.store(meta_props=meta_prop_sw_spec)\nsw_spec_uid = client.software_specifications.get_uid(sw_spec_details)\n\n# Adicionando o pacote customizado \u00e0 nova especifica\u00e7\u00e3o\nclient.software_specifications.add_package_extension(sw_spec_uid, pkg_extn_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Finalmente, vamos publicar a pipeline utilizando a especifica\u00e7\u00e3o de software customizada que criamos."}, {"metadata": {}, "cell_type": "code", "source": "# Metadados do modelo\nmodel_props = {\n    client.repository.ModelMetaNames.NAME: \"Modelo com Pipeline customizada\",\n    client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23',\n    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sw_spec_uid\n}\n\n# Publicando a Pipeline como um modelo\npublished_model = client.repository.store_model(model=pipeline, meta_props=model_props)\npublished_model_uid = client.repository.get_model_uid(published_model)\nclient.repository.get_details(published_model_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Seu modelo agora est\u00e1 salvo. Vamos agora deix\u00e1-lo dispon\u00edvel online, para que possamos test\u00e1-lo:"}, {"metadata": {}, "cell_type": "code", "source": "# Metadados para publica\u00e7\u00e3o do modelo\nmetadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"Publica\u00e7\u00e3o do modelo customizado\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\n# Publicar\ncreated_deployment = client.deployments.create(published_model_uid, meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Parab\u00e9ns!\n\nSeu modelo est\u00e1 agora publicado. Quando estiver pronto para submeter o desafio, voc\u00ea pode acessar https://maratona.dev/challenge/1, e utilizar as credenciais abaixo para realizar a submiss\u00e3o. Lembre-se de revisar todas as instru\u00e7\u00f5es no [README](https://github.com/maratonadev/desafio-1-2021) antes de submeter!"}, {"metadata": {}, "cell_type": "code", "source": "deployment_uid = client.deployments.get_uid(created_deployment)\n\nprint(f\"Credenciais para envio (n\u00e3o compartilhe esses dados com ningu\u00e9m!)\\n\\nAPI key: {api_key}\\nDeployment ID: {deployment_uid}\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}