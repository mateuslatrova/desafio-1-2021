{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Desaf\u00edo 1 - Marat\u00f3n Behind the Code 2021\n\n### Notebook guia\n\nEste Jupyter Notebook te dar\u00e1 instrucciones para crear una soluci\u00f3n introductoria al Desaf\u00edo 1 de la Marat\u00f3n. \u00a1Si\u00e9ntete libre de editar y mejorar tu soluci\u00f3n!"}, {"metadata": {}, "cell_type": "markdown", "source": "**Ten en cuenta: si est\u00e1s utilizando Watson Studio, recuerda hacer que tu Notebook sea editable haciendo clic en el bot\u00f3n de edici\u00f3n de arriba.**\n\n![](https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/edit-notebook.png)"}, {"metadata": {}, "cell_type": "markdown", "source": "## Exploraci\u00f3n de dataset\nEl primer paso para desarrollar un buen modelo de aprendizaje autom\u00e1tico es explorar los datos con los que tenemos que trabajar. Debemos comprender lo mejor posible la relevancia de cada dato para el valor que queremos predecir. Despu\u00e9s de todo, la predicci\u00f3n del modelo se basa completamente en los datos con los que se entren\u00f3.\n\nHay muchas bibliotecas de Python que se pueden utilizar para el procesamiento y la visualizaci\u00f3n de datos. En este caso usaremos Pandas, Seaborn y Matplotlib."}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Primero, cargamos el conjunto de datos del desaf\u00edo en este notebook. Comencemos con el principal, `LOANS.csv`. Para eso, podemos usar el \u00edcono de activos, disponible en la esquina superior derecha de la pantalla, e insertar el conjunto de datos como un DataFrame Pandas, como en la imagen de abajo.\n\n<img width=\"300px\" src=\"https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/load-loans.png\" />\n\nRepite este procedimiento para todos los datasets que vas a utilizar."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Cargue aqui el dataset", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Cambia el nombre de la variable creada con el conjunto de datos para `loans`, para cumplir con los c\u00f3digos a continuaci\u00f3n."}, {"metadata": {}, "cell_type": "code", "source": "loans = df_cargado", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Podemos usar los m\u00e9todos .info () y .describe () para obtener informaci\u00f3n b\u00e1sica sobre la cantidad actual de datos, sus tipos y valores."}, {"metadata": {}, "cell_type": "code", "source": "loans.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "loans.describe()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "La variable de destino para este desaf\u00edo es `ALLOW`, es decir, si se debe permitir o no un pr\u00e9stamo, seg\u00fan la informaci\u00f3n proporcionada. Echemos un vistazo a c\u00f3mo se distribuye esta variable."}, {"metadata": {}, "cell_type": "code", "source": "risk_plot = sns.countplot(data=loans, x='ALLOW', order=loans['ALLOW'].value_counts().index)\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Si\u00e9ntete libre de ver la distribuci\u00f3n de otras columnas en el conjunto de datos, usar los otros conjuntos de datos, explorar correlaciones entre variables y m\u00e1s."}, {"metadata": {}, "cell_type": "code", "source": "# Adicione suas explora\u00e7\u00f5es", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Procesamiento de datos"}, {"metadata": {}, "cell_type": "markdown", "source": "Una vez que hemos explorado los datos, comprendemos la importancia de cada columna y podemos hacer cambios en ellas para obtener un mejor resultado. Aqu\u00ed, vamos a hacer un procesamiento simple, para eliminar del conjunto de datos las l\u00edneas a las que les falta alg\u00fan valor. Esta t\u00e9cnica no es necesariamente la mejor para usar en el desaf\u00edo, es solo un ejemplo de c\u00f3mo manejar el conjunto de datos.\n\nPara procesamientos m\u00e1s avanzados, como modificar columnas o crear columnas nuevas, ve a continuaci\u00f3n en Notebook, donde explicamos c\u00f3mo usar `Pipelines`, desde la biblioteca `sklearn`, para realizar transformaciones de datos."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "clean_df = loans.dropna()\nclean_df.count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Podemos observar que ahora temos un dataset \"limpio\", pero perdimos algunos datos al eliminar filas donde faltaba al menos una columna."}, {"metadata": {}, "cell_type": "markdown", "source": "Al observar la ejecuci\u00f3n del m\u00e9todo `.info()` anterior, podemos ver que hay tres columnas de tipo `object`. El modelo `scikit-learn` que vamos a utilizar no es capaz de procesar dicha variable. Entonces, para continuar con el experimento, eliminemos esta columna. Recomendamos que utilice alguna t\u00e9cnica para manejar variables categ\u00f3ricas, como _one-hot encoding_, en lugar de eliminar la columna.\n\nTambi\u00e9n eliminaremos la columna `ID`, ya que sabemos que no es informaci\u00f3n \u00fatil para la predicci\u00f3n (es solo un n\u00famero que identifica a un cliente)."}, {"metadata": {}, "cell_type": "code", "source": "object_columns = ['INSTALLMENT_PLANS', 'LOAN_PURPOSE', 'OTHERS_ON_LOAN']\nclean_df = clean_df.drop(object_columns, axis=1)\nclean_df = clean_df.drop('ID', axis=1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clean_df.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Creaci\u00f3n del modelo\n\nCon los datos listos, podemos seleccionar un modelo de Machine Learning para entrenar con nuestros datos. En este ejemplo, vamos a utilizar un modelo de clasificaci\u00f3n b\u00e1sico, el \u00e1rbol de decisiones.\n\nPara poder evaluar el rendimiento de nuestro modelo, dividamos los datos que tenemos entre los datos de entrenamiento y de prueba, y luego, despu\u00e9s del entrenamiento, veremos c\u00f3mo le va con las predicciones."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "A continuaci\u00f3n, separamos los datos que queremos predecir de los datos que usamos como informaci\u00f3n para la predicci\u00f3n."}, {"metadata": {}, "cell_type": "code", "source": "features = ['PAYMENT_TERM', 'INSTALLMENT_PERCENT', 'LOAN_AMOUNT']\ntarget = ['ALLOW']\n\nX = clean_df[features]\ny = clean_df[target]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "test_pct = 0.3 # Separaremos 30% de los dados para testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_pct)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(f\"Exactitud del modelo (n\u00famero de predicciones asertadas sobre el n\u00famero total de pruebas): {acc}\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Aunque estamos usando solo unas pocas variables del conjunto de datos cargado, el desaf\u00edo espera un modelo que acepte todas las variables de los conjuntos de datos disponibles. Entonces, usemos un transformador para transformar los datos de entrada, eliminando las columnas que no queremos, antes de enviarlo a nuestro modelo. Por lo tanto, vamos a crear un `Pipeline`, que usa el transformador como entrada, y nuestro modelo a continuaci\u00f3n.\n\nDepende de ti unir los otros conjuntos de datos disponibles y usarlos tambi\u00e9n para las predicciones en el modelo, en lugar de eliminar las columnas."}, {"metadata": {}, "cell_type": "markdown", "source": "## Sobre Pipelines\n\nUn `Pipeline`, de la librer\u00eda `scikit-learn`, consta de una serie de pasos donde realizamos transformaciones de datos. Las transformaciones est\u00e1n definidas por clases que siempre deben tener **dos m\u00e9todos**:\n\n- **fit**: Un m\u00e9todo que recibe datos de entrenamiento y devuelve la propia instancia de clase. Se aplica cuando se entrena para usar una canalizaci\u00f3n para entrenar un modelo.\n- **transform**: Un m\u00e9todo que toma un conjunto de datos como entrada y debe devolver otro conjunto de datos, transformado. Se aplica a cada paso del Pipeline, recibiendo los datos del paso anterior y transform\u00e1ndolos.\n\nVea a continuaci\u00f3n una representaci\u00f3n gr\u00e1fica de c\u00f3mo funciona un Pipeline:"}, {"metadata": {}, "cell_type": "markdown", "source": "![](https://s3.br-sao.cloud-object-storage.appdomain.cloud/maratona-static/pipeline-es.png)"}, {"metadata": {}, "cell_type": "markdown", "source": "En este Notebook, creamos un Pipeline muy similar al ejemplo anterior, con dos etapas:\n\n- **drop_columns**: Remueve las columnas indeseadas del conjunto de datos de entrada.\n- **classification**: Alimenta un modelo de clasificaci\u00f3n con los datos obtenidos de la etapa de **drop_columns**, pudiendo ser tanto para entrenamiento como para obtener una predicci\u00f3n. "}, {"metadata": {}, "cell_type": "markdown", "source": "## Creaci\u00f3n de Pipelines con scikit-learn\n\nPara crear un modelo capaz de hacer transformaciones con los datos de entrada, vamos a crear un `Pipeline` de `scikit-learn` y aplicar nuestras transformaciones dentro de sus etapas.\n\nA continuaci\u00f3n, definimos un transformador de ejemplo, que eliminar\u00e1 las columnas pasadas como par\u00e1metro en su inicializaci\u00f3n:"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\n\n# Un transformador para remover columnas indeseadas\nclass DropColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Primero realizamos la c\u00f3pia del DataFrame 'X' de entrada\n        data = X.copy()\n        # Retornamos um nuevo dataframe sin las colunmas indeseadas\n        return data.drop(labels=self.columns, axis='columns')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Tanto el m\u00e9todo `fit` como `transform` deben definirse, incluso si no van a hacer nada diferente, como en el caso de ese `fit`.\n\nAsimismo, puedes crear otros transformadores, para otros fines, siempre heredando de las clases `BaseEstimator` y `TransformerMixin`. Puedes utilizar un transformador para, por ejemplo, crear nuevas columnas, editar tipos de datos de columnas existentes, etc.\n\nAhora, vamos a crear un Pipeline para utilizar nuestro modelo, conservando todas las columnas esperadas para el desaf\u00edo y removiendo las que no queremos usar."}, {"metadata": {}, "cell_type": "code", "source": "challenge_columns = ['ID', 'CHECKING_BALANCE', 'PAYMENT_TERM', 'CREDIT_HISTORY',\n       'LOAN_PURPOSE', 'LOAN_AMOUNT', 'EXISTING_SAVINGS',\n       'EMPLOYMENT_DURATION', 'INSTALLMENT_PERCENT', 'SEX', 'OTHERS_ON_LOAN',\n       'CURRENT_RESIDENCE_DURATION', 'PROPERTY', 'AGE', 'INSTALLMENT_PLANS',\n       'HOUSING', 'EXISTING_CREDITS_COUNT', 'JOB_TYPE', 'DEPENDENTS',\n       'TELEPHONE', 'FOREIGN_WORKER', 'ALLOW']\n\nunwanted_columns = list((set(challenge_columns) - set(target)) - set(features)) # Remover todas las colunmas que no son features do nuestro modelo", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Creando una instancia del transformador, pasando como par\u00e1metro las colunmas que no queremos\ndrop_columns = DropColumns(unwanted_columns)\n\n\n# Creando un Pipeline, adicionando nuestro transformador seguido de un modelo de \u00e1rbol de decisi\u00f3n\nskl_pipeline = Pipeline(steps=[('drop_columns', drop_columns), ('classification', model)])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\u00a1Listo! Este Pipeline ahora est\u00e1 listo para recibir todas las variables de desaf\u00edo, aunque el modelo solo use algunas."}, {"metadata": {}, "cell_type": "markdown", "source": "## Deploy del modelo para Watson Machine Learning (WML)\n\nAhora que tenemos el modelo listo para su publicaci\u00f3n, queremos ponerlo en l\u00ednea para que el sistema de la Marat\u00f3n pueda probarlo :)\n\nPara ello, utilizaremos la biblioteca `IBM Watson Machine Learning`, que le permite encapsular modelos de Machine Learning en APIs REST."}, {"metadata": {}, "cell_type": "code", "source": "# Instalar la biblioteca WML\n!pip install -U ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Si a\u00fan no lo ha creado, cree un servicio de Machine Learning aqu\u00ed https://cloud.ibm.com/catalog/services/machine-learning.\n\nIngresa tus credenciales para el servicio en la celda a continuaci\u00f3n.\n\nEn `location`, ingresa el ID de la regi\u00f3n donde se encuentra su servicio de WML instanciado, de acuerdo con las posibilidades de abajo:\n\n-\tDallas - `us-south`\n-\tLondon - `eu-gb`\n-\tFrankfurt - `eu-de`\n-\tTokyo - `jp-tok`\n\nPara la API key, generela aqui: https://cloud.ibm.com/iam/apikeys. No la compartas con nadie! Una API key da acesso a su cuenta de IBM Cloud."}, {"metadata": {}, "cell_type": "code", "source": "api_key = 'INSIRA SUA API KEY AQUI'\nlocation = 'us-south' # En caso de WML estar en una regi\u00f3n diferente, altere esta linea\n\nwml_credentials = {\n    \"apikey\": api_key,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Crea un espacio para guardar tu modelo. Puedes crearlo aqui: https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas\n\nCuando crees tu espacio, **asocia la instancia de tu servicio de WML al espacio!** Sin asociar, no conseguiras efectuar el deploy."}, {"metadata": {}, "cell_type": "code", "source": "# Lista espacios creados en su instancia de WML\nclient.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Copia el ID de tu espacio creado para el desaf\u00edo y p\u00e9gualo a continuaci\u00f3n para usarlo. Deber\u00edas ver el mensaje 'SUCCESS' si el espacio est\u00e1 configurado correctamente."}, {"metadata": {}, "cell_type": "code", "source": "space_id = 'cole aqui'\nclient.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Utilizaci\u00f3n de Pipeline dentro de Watson Machine Learning (WML)\n\nPara utilizar un Pipeline en WML con transformadores customizados, son necesarios algunos pasos adicionales:\n\n1.\tCrea un paquete en Python que contenga el transformador personalizado.\n2.\tCarga ese paquete con el transformador en un reposit\u00f3rio en WML;\n3.\tCrea una especificaci\u00f3n de software, con este paquete personalizado, que se utilizar\u00e1 como tiempo de ejecuci\u00f3n del modelo en WML.\n\nComo exemplo, vamos utilizar un paquete ya listo, disponible aqui: https://github.com/vnderlev/watson-sklearn-transforms. Para configurar un paquete de Python, son necesarios algunos otros archivos, pero la l\u00f3gica del transformador creado se encuentra en [este archivo](https://github.com/vnderlev/watson-sklearn-transforms/blob/master/my_custom_sklearn_transforms/sklearn_transformers.py). En este caso, este es el mismo transformador que definimos aqu\u00ed, excluir\u00e1 del conjunto de datos las columnas pasadas como par\u00e1metros en su inicializaci\u00f3n.\n\nAbajo, vamos a bajar este paquete de GitHub e instalarlo en Python."}, {"metadata": {}, "cell_type": "code", "source": "!rm -rf watson-sklearn-transforms # Remover carpeta en caso de que ya exista\n!git clone https://github.com/vnderlev/watson-sklearn-transforms # Clonar el reposit\u00f3rio con el pacote\n!zip -r drop-columns.zip watson-sklearn-transforms # Zippear el paquete\n!pip install drop-columns.zip # Instalar el paquete zippeado", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Vamos ahora a recrear nuestro Pipeline utilizando este paquete instalado."}, {"metadata": {}, "cell_type": "code", "source": "from my_custom_sklearn_transforms.sklearn_transformers import DropColumns\n\ndrop_columns = DropColumns(unwanted_columns)\n\npipeline = Pipeline(steps=[('drop_columns', drop_columns), ('classification', model)])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Vamos ahora a subir el transformador customizado que bajamos para WML."}, {"metadata": {}, "cell_type": "code", "source": "# Metadatos para el paquete customizado\nmeta_prop_pkg_extn = {\n    client.package_extensions.ConfigurationMetaNames.NAME: \"Drop_Columns\",\n    client.package_extensions.ConfigurationMetaNames.DESCRIPTION: \"Extensi\u00f3n para remover columnas\",\n    client.package_extensions.ConfigurationMetaNames.TYPE: \"pip_zip\"\n}\n\n# Subir el paquete\npkg_extn_details = client.package_extensions.store(meta_props=meta_prop_pkg_extn, file_path=\"drop-columns.zip\")\n\n# Guardar las informaciones sobre el paquete\npkg_extn_uid = client.package_extensions.get_uid(pkg_extn_details)\npkg_extn_url = client.package_extensions.get_href(pkg_extn_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Ahora creemos una especificaci\u00f3n de software con nuestro paquete personalizado para que lo use WML. Si est\u00e1s utilizando un software que no sea `Python 3.8` o una biblioteca que no sea `scikit-learn`, puedes consultar la lista de especificaciones de software compatibles con WML: https://dataplatform.cloud.ibm.com/docs/content/wsj/wmls/wmls-deploy-python-types.html?context=analytics&audience=wdp"}, {"metadata": {}, "cell_type": "code", "source": "base_sw_spec_uid = client.software_specifications.get_uid_by_name(\"default_py3.8\")\n\n# Si desea utilizar un software que no sea Python 3.8 como base, eche un vistazo a los disponibles con la l\u00ednea a continuaci\u00f3n\n# client.software_specifications.list(limit=100)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Metadatos de la nueva especificaci\u00f3n de software\nmeta_prop_sw_spec = {\n    client.software_specifications.ConfigurationMetaNames.NAME: \"sw_spec_drop_columns\",\n    client.software_specifications.ConfigurationMetaNames.DESCRIPTION: \"Software specification for DropColumns\",\n    client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {\"guid\": base_sw_spec_uid}\n}\n\n# Creando la nueva especificacion de software y obteniendo su ID\nsw_spec_details = client.software_specifications.store(meta_props=meta_prop_sw_spec)\nsw_spec_uid = client.software_specifications.get_uid(sw_spec_details)\n\n# Agregar el paquete personalizado a la nueva especificaci\u00f3n\nclient.software_specifications.add_package_extension(sw_spec_uid, pkg_extn_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Finalmente, vamos a publicar el pipeline utilizando la especificaci\u00f3n de software customizada que creamos."}, {"metadata": {}, "cell_type": "code", "source": "# Metadatos del modelo\nmodel_props = {\n    client.repository.ModelMetaNames.NAME: \"Modelo com Pipeline customizada\",\n    client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23',\n    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sw_spec_uid\n}\n\n# Publicando el Pipeline como um modelo\npublished_model = client.repository.store_model(model=pipeline, meta_props=model_props)\npublished_model_uid = client.repository.get_model_uid(published_model)\nclient.repository.get_details(published_model_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Su modelo ahora est\u00e1 guardado. Vamos ahora a dejarlo disponible online, para que podamos testearlo:"}, {"metadata": {}, "cell_type": "code", "source": "# Metadatos para publicaci\u00f3n del modelo\nmetadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"Publicaci\u00f3n de modelo customizado\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\n# Publicar\ncreated_deployment = client.deployments.create(published_model_uid, meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Felicitaciones!\n\nSu modelo est\u00e1 ahora publicado. Cuando este listo para enviar el desafio, puedes acceder a https://maratona.dev/challenge/1, y utilizar las credenciales abajo para realizar la entrega. Recuerda revisar todas las instrucciones en el [README](https://github.com/maratonadev/desafio-1-2021) antes de entregar!"}, {"metadata": {}, "cell_type": "code", "source": "deployment_uid = client.deployments.get_uid(created_deployment)\n\nprint(f\"Credenciales para el envio (no compartir estos datos con nadie!)\\n\\nAPI key: {api_key}\\nDeployment ID: {deployment_uid}\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}